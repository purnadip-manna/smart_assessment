{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!curl -LO https://github.com/purnadip-manna/smart_assessment/raw/main/models/testmodel1.h5\n!curl -LO https://raw.githubusercontent.com/purnadip-manna/smart_assessment/main/data.csv","metadata":{"execution":{"iopub.status.busy":"2022-01-12T06:11:24.804418Z","iopub.execute_input":"2022-01-12T06:11:24.804986Z","iopub.status.idle":"2022-01-12T06:11:26.91046Z","shell.execute_reply.started":"2022-01-12T06:11:24.804945Z","shell.execute_reply":"2022-01-12T06:11:26.909648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install sentence-transformers ","metadata":{"execution":{"iopub.status.busy":"2022-01-12T06:11:26.913162Z","iopub.execute_input":"2022-01-12T06:11:26.913624Z","iopub.status.idle":"2022-01-12T06:11:34.25653Z","shell.execute_reply.started":"2022-01-12T06:11:26.913573Z","shell.execute_reply":"2022-01-12T06:11:34.255708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install keybert","metadata":{"execution":{"iopub.status.busy":"2022-01-12T06:11:34.259904Z","iopub.execute_input":"2022-01-12T06:11:34.260145Z","iopub.status.idle":"2022-01-12T06:11:41.884624Z","shell.execute_reply.started":"2022-01-12T06:11:34.260117Z","shell.execute_reply":"2022-01-12T06:11:41.883788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport transformers\nimport csv\nimport pandas as pd\nfrom keras.models import load_model \nimport pandas as pd   \nimport sklearn \nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.metrics.pairwise import manhattan_distances\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom sklearn import metrics  \nfrom scipy.spatial import distance\nfrom sentence_transformers import SentenceTransformer \nfrom keybert import KeyBERT\nkw_model = KeyBERT()\nkey_model = SentenceTransformer('distilbert-base-nli-mean-tokens')","metadata":{"execution":{"iopub.status.busy":"2022-01-12T06:11:41.888668Z","iopub.execute_input":"2022-01-12T06:11:41.888913Z","iopub.status.idle":"2022-01-12T06:12:09.971138Z","shell.execute_reply.started":"2022-01-12T06:11:41.888881Z","shell.execute_reply":"2022-01-12T06:12:09.970329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\") \nmax_length = 128  # Maximum length of input sentence to the model.\nbatch_size = 32\nlabels = [\"contradiction\", \"entailment\", \"neutral\"]","metadata":{"execution":{"iopub.status.busy":"2022-01-12T06:12:09.974515Z","iopub.execute_input":"2022-01-12T06:12:09.974832Z","iopub.status.idle":"2022-01-12T06:12:11.863529Z","shell.execute_reply.started":"2022-01-12T06:12:09.974801Z","shell.execute_reply":"2022-01-12T06:12:11.862838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BertSemanticDataGenerator(tf.keras.utils.Sequence): \n    def __init__(\n        self,\n        sentence_pairs,\n        labels,\n        batch_size=batch_size,\n        shuffle=True,\n        include_targets=True,\n    ):\n        self.sentence_pairs = sentence_pairs\n        self.labels = labels\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n        self.include_targets = include_targets\n        \n        # Load our BERT Tokenizer to encode the text.\n        # We will use base-base-uncased pretrained model.\n        \n        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n            \"bert-base-uncased\", do_lower_case=True\n        )\n        self.indexes = np.arange(len(self.sentence_pairs))\n        self.on_epoch_end()\n\n    def __len__(self):\n        # Denotes the number of batches per epoch.\n        return len(self.sentence_pairs) // self.batch_size\n\n    def __getitem__(self, idx):\n        # Retrieves the batch of index.\n        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n        sentence_pairs = self.sentence_pairs[indexes]\n\n        # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n        # encoded together and separated by [SEP] token.\n        encoded = self.tokenizer.batch_encode_plus(\n            sentence_pairs.tolist(),\n            add_special_tokens=True,\n            max_length=max_length,\n            return_attention_mask=True,\n            return_token_type_ids=True,\n            pad_to_max_length=True,\n            return_tensors=\"tf\",\n        )   \n\n        bert_output = bert_model(**encoded)\n        sequence_output = bert_output.last_hidden_state\n         \n        if self.include_targets:\n            labels = np.array(self.labels[indexes], dtype=\"int32\")\n            return sequence_output, labels\n        else:\n            return sequence_output\n\n    def on_epoch_end(self):\n        # Shuffle indexes after each epoch if shuffle is set to True.\n        if self.shuffle:\n            np.random.RandomState(42).shuffle(self.indexes)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-12T06:12:11.864901Z","iopub.execute_input":"2022-01-12T06:12:11.865306Z","iopub.status.idle":"2022-01-12T06:12:11.877143Z","shell.execute_reply.started":"2022-01-12T06:12:11.865251Z","shell.execute_reply":"2022-01-12T06:12:11.876379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SimilarityMetric:\n    def __init__(self,student_answer,standard_answer,st_ans,sn_ans,key_model=key_model) -> None:\n        self.student_answer=student_answer\n        self.standard_answer=standard_answer \n        self.st_ans=st_ans\n        self.sn_ans=sn_ans \n        self.embded_student_answer = key_model.encode(student_answer)\n        self.embded_standard_answer= key_model.encode(standard_answer)\n\n    def euclidian_dist(self): \n        dist=euclidean_distances(self.embded_standard_answer,self.embded_student_answer) \n        result=0.0\n        for d in dist: \n            result=min(d)\n        return result/dist.shape[0]\n    \n    def manhatten_dist(self):\n        dist=manhattan_distances( self.embded_standard_answer,self.embded_student_answer) \n        result=0.0\n        for d in dist: \n            result=min(d)\n        return result/dist.shape[0]\n#         return manhattan_distances( self.embded_standard_answer,self.embded_student_answer) \n\n    def cosine_similarity(self): \n        distances = cosine_similarity( self.embded_standard_answer,self.embded_student_answer)\n        result=0.0\n        for d in distances: \n            result=max(d)\n        return result/distances.shape[0] \n    \n    def Jaccard_Similarity(self): \n\n        words_doc1 = set(self.st_ans.lower().split()) \n        words_doc2 = set(self.sn_ans.lower().split())\n\n        intersection = words_doc1.intersection(words_doc2)\n\n        union = words_doc1.union(words_doc2)\n\n        return float(len(intersection)) / len(union)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-12T06:12:11.878485Z","iopub.execute_input":"2022-01-12T06:12:11.878751Z","iopub.status.idle":"2022-01-12T06:12:11.891408Z","shell.execute_reply.started":"2022-01-12T06:12:11.878716Z","shell.execute_reply":"2022-01-12T06:12:11.890733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=load_model('./testmodel1.h5')","metadata":{"execution":{"iopub.status.busy":"2022-01-12T06:12:11.892802Z","iopub.execute_input":"2022-01-12T06:12:11.893196Z","iopub.status.idle":"2022-01-12T06:12:12.392133Z","shell.execute_reply.started":"2022-01-12T06:12:11.893159Z","shell.execute_reply":"2022-01-12T06:12:12.391396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('data.csv')\nlength = len(df.index)\nfor i in range(length):\n    question = df.loc[i, \"questions\"]\n    sentence1 = df.loc[i, \"sentence1\"]\n    sentence2 = df.loc[i, \"sentence2\"]\n    label = df.loc[i, \"label\"]\n\n    sentence_pairs = np.array([[str(sentence1), str(sentence2)]])\n    test_data = BertSemanticDataGenerator(\n        sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n    )\n    result = model.predict(test_data)\n    index = np.argmax(result, axis=None)\n    pred_label = labels[index]\n    percentage = str(result[0][index])\n    sucess = 0\n    if(pred_label.strip() == label.strip()):\n        sucess = 1\n        \n#     Keyword ..\n\n    keywords_stnd = kw_model.extract_keywords([sentence1], keyphrase_ngram_range=(1, 3), stop_words='english', use_mmr=True, diversity=0.3) \n    keywords_stud = kw_model.extract_keywords([sentence2], keyphrase_ngram_range=(1, 3), stop_words='english', use_mmr=True, diversity=0.3)\n    candidates_standard=[]\n    candidates_student=[]\n\n    for key,match in keywords_stnd[0]: \n      candidates_standard.append(key)\n    for key,match in keywords_stud[0]:\n      candidates_student.append(key) \n\n    sim_obj=SimilarityMetric(candidates_standard,candidates_student,str(sentence1),str(sentence2))\n    \n    cosine_similar=sim_obj.cosine_similarity()\n    euclid_dist=sim_obj.euclidian_dist()\n    manhatten_d=sim_obj.manhatten_dist()\n    jaccard_Similarity=sim_obj.Jaccard_Similarity()\n    \n    \n    with open('result.csv', 'a', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([question, sentence1, sentence2, label, pred_label, percentage, sucess, str(result),cosine_similar,jaccard_Similarity,euclid_dist,manhatten_d])\n","metadata":{"execution":{"iopub.status.busy":"2022-01-12T06:12:12.39362Z","iopub.execute_input":"2022-01-12T06:12:12.393857Z","iopub.status.idle":"2022-01-12T06:12:18.775264Z","shell.execute_reply.started":"2022-01-12T06:12:12.393824Z","shell.execute_reply":"2022-01-12T06:12:18.774069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}