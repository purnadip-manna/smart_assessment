{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"!pip install transformers\n!curl -LO https://raw.githubusercontent.com/MohamadMerchant/SNLI/master/data.tar.gz\n!tar -xvzf data.tar.gz","metadata":{"execution":{"iopub.status.busy":"2022-01-10T17:05:52.027352Z","iopub.execute_input":"2022-01-10T17:05:52.028063Z","iopub.status.idle":"2022-01-10T17:06:04.635106Z","shell.execute_reply.started":"2022-01-10T17:05:52.028013Z","shell.execute_reply":"2022-01-10T17:06:04.633774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport transformers","metadata":{"execution":{"iopub.status.busy":"2022-01-10T17:06:04.638541Z","iopub.execute_input":"2022-01-10T17:06:04.638951Z","iopub.status.idle":"2022-01-10T17:06:04.645841Z","shell.execute_reply.started":"2022-01-10T17:06:04.638894Z","shell.execute_reply":"2022-01-10T17:06:04.644645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\") \nbert_model.trainable = False\nmax_length = 128  # Maximum length of input sentence to the model.\nbatch_size = 32\nepochs = 2\n\n# Labels in our dataset.\nlabels = [\"contradiction\", \"entailment\", \"neutral\"]","metadata":{"execution":{"iopub.status.busy":"2022-01-10T17:06:04.648395Z","iopub.execute_input":"2022-01-10T17:06:04.649278Z","iopub.status.idle":"2022-01-10T17:06:07.024984Z","shell.execute_reply.started":"2022-01-10T17:06:04.649162Z","shell.execute_reply":"2022-01-10T17:06:07.023941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There are more than 550k samples in total; we will use 100k for this example.\ntrain_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_train.csv\", nrows=200000) # 20k\nvalid_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_dev.csv\")   # 10k\ntest_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_test.csv\")\n\n# Shape of the data\nprint(f\"Total train samples : {train_df.shape[0]}\")\nprint(f\"Total validation samples: {valid_df.shape[0]}\")\nprint(f\"Total test samples: {valid_df.shape[0]}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-10T17:06:07.027546Z","iopub.execute_input":"2022-01-10T17:06:07.028017Z","iopub.status.idle":"2022-01-10T17:06:07.614849Z","shell.execute_reply.started":"2022-01-10T17:06:07.027976Z","shell.execute_reply":"2022-01-10T17:06:07.613934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Sentence1: {train_df.loc[1, 'sentence1']}\")\nprint(f\"Sentence2: {train_df.loc[1, 'sentence2']}\")\nprint(f\"Similarity: {train_df.loc[1, 'similarity']}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-10T17:06:07.616471Z","iopub.execute_input":"2022-01-10T17:06:07.617564Z","iopub.status.idle":"2022-01-10T17:06:07.648935Z","shell.execute_reply.started":"2022-01-10T17:06:07.617517Z","shell.execute_reply":"2022-01-10T17:06:07.64802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We have some NaN entries in our train data, we will simply drop them.\nprint(\"Number of missing values\")\nprint(train_df.isnull().sum())\ntrain_df.dropna(axis=0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T17:06:07.653655Z","iopub.execute_input":"2022-01-10T17:06:07.656511Z","iopub.status.idle":"2022-01-10T17:06:07.838952Z","shell.execute_reply.started":"2022-01-10T17:06:07.656464Z","shell.execute_reply":"2022-01-10T17:06:07.837897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Train Target Distribution\")\nprint(train_df.similarity.value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-01-10T17:06:07.840571Z","iopub.execute_input":"2022-01-10T17:06:07.841198Z","iopub.status.idle":"2022-01-10T17:06:07.873493Z","shell.execute_reply.started":"2022-01-10T17:06:07.841152Z","shell.execute_reply":"2022-01-10T17:06:07.87239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Validation Target Distribution\")\nprint(valid_df.similarity.value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-01-10T17:06:07.875029Z","iopub.execute_input":"2022-01-10T17:06:07.875487Z","iopub.status.idle":"2022-01-10T17:06:07.887072Z","shell.execute_reply.started":"2022-01-10T17:06:07.87544Z","shell.execute_reply":"2022-01-10T17:06:07.885964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = (\n    train_df[train_df.similarity != \"-\"]\n    .sample(frac=1.0, random_state=42)\n    .reset_index(drop=True)\n)\nvalid_df = (\n    valid_df[valid_df.similarity != \"-\"]\n    .sample(frac=1.0, random_state=42)\n    .reset_index(drop=True)\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T17:06:07.888854Z","iopub.execute_input":"2022-01-10T17:06:07.891038Z","iopub.status.idle":"2022-01-10T17:06:07.973725Z","shell.execute_reply.started":"2022-01-10T17:06:07.891005Z","shell.execute_reply":"2022-01-10T17:06:07.972628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"label\"] = train_df[\"similarity\"].apply(\n    lambda x: 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n)\ny_train = tf.keras.utils.to_categorical(train_df.label, num_classes=3)\n\nvalid_df[\"label\"] = valid_df[\"similarity\"].apply(\n    lambda x: 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n)\ny_val = tf.keras.utils.to_categorical(valid_df.label, num_classes=3)\n\ntest_df[\"label\"] = test_df[\"similarity\"].apply(\n    lambda x: 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n)\ny_test = tf.keras.utils.to_categorical(test_df.label, num_classes=3)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T17:06:07.9782Z","iopub.execute_input":"2022-01-10T17:06:07.978537Z","iopub.status.idle":"2022-01-10T17:06:08.133345Z","shell.execute_reply.started":"2022-01-10T17:06:07.978495Z","shell.execute_reply":"2022-01-10T17:06:08.132191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BertSemanticDataGenerator(tf.keras.utils.Sequence): \n    def __init__(\n        self,\n        sentence_pairs,\n        labels,\n        batch_size=batch_size,\n        shuffle=True,\n        include_targets=True,\n    ):\n        self.sentence_pairs = sentence_pairs\n        self.labels = labels\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n        self.include_targets = include_targets\n        \n        # Load our BERT Tokenizer to encode the text.\n        # We will use base-base-uncased pretrained model.\n        \n        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n            \"bert-base-uncased\", do_lower_case=True\n        )\n        self.indexes = np.arange(len(self.sentence_pairs))\n        self.on_epoch_end()\n\n    def __len__(self):\n        # Denotes the number of batches per epoch.\n        return len(self.sentence_pairs) // self.batch_size\n\n    def __getitem__(self, idx):\n        # Retrieves the batch of index.\n        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n        sentence_pairs = self.sentence_pairs[indexes]\n\n        # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n        # encoded together and separated by [SEP] token.\n        encoded = self.tokenizer.batch_encode_plus(\n            sentence_pairs.tolist(),\n            add_special_tokens=True,\n            max_length=max_length,\n            return_attention_mask=True,\n            return_token_type_ids=True,\n            pad_to_max_length=True,\n            return_tensors=\"tf\",\n        )   \n\n        bert_output = bert_model(**encoded)\n        \n        sequence_output = bert_output.last_hidden_state\n#         pooled_output = bert_output.pooler_output\n         \n        if self.include_targets:\n            labels = np.array(self.labels[indexes], dtype=\"int32\")\n            return sequence_output, labels\n        else:\n            return sequence_output\n\n    def on_epoch_end(self):\n        # Shuffle indexes after each epoch if shuffle is set to True.\n        if self.shuffle:\n            np.random.RandomState(42).shuffle(self.indexes)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T17:06:08.135173Z","iopub.execute_input":"2022-01-10T17:06:08.135541Z","iopub.status.idle":"2022-01-10T17:06:08.150315Z","shell.execute_reply.started":"2022-01-10T17:06:08.135497Z","shell.execute_reply":"2022-01-10T17:06:08.149189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"strategy = tf.distribute.MirroredStrategy()\n\nwith strategy.scope(): \n    input_layer = tf.keras.layers.Input(shape=(128, 768), name=None)\n#     input_layer=tf.reshape(input_layer, (128, 768), name=None) \n    \n    bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(input_layer) \n    \n    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n    max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n    concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n    dropout = tf.keras.layers.Dropout(0.3)(concat)    \n    output = tf.keras.layers.Dense(3, activation=\"softmax\")(dropout)\n    model = tf.keras.models.Model(\n        inputs=input_layer, outputs=output\n    )\n    \n    model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-10T17:06:08.15215Z","iopub.execute_input":"2022-01-10T17:06:08.152755Z","iopub.status.idle":"2022-01-10T17:06:09.032925Z","shell.execute_reply.started":"2022-01-10T17:06:08.152706Z","shell.execute_reply":"2022-01-10T17:06:09.031906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(\n        optimizer=tf.keras.optimizers.Adam(),\n        loss=\"categorical_crossentropy\",\n        metrics=[\"acc\"],\n    )","metadata":{"execution":{"iopub.status.busy":"2022-01-10T17:06:09.036517Z","iopub.execute_input":"2022-01-10T17:06:09.036798Z","iopub.status.idle":"2022-01-10T17:06:09.05523Z","shell.execute_reply.started":"2022-01-10T17:06:09.036767Z","shell.execute_reply":"2022-01-10T17:06:09.053972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = BertSemanticDataGenerator(\n    train_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"),\n    y_train,\n    batch_size=batch_size,\n    shuffle=True,\n)\nvalid_data = BertSemanticDataGenerator(\n    valid_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"),\n    y_val,\n    batch_size=batch_size,\n    shuffle=False,\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T17:06:09.056773Z","iopub.execute_input":"2022-01-10T17:06:09.057072Z","iopub.status.idle":"2022-01-10T17:06:16.785826Z","shell.execute_reply.started":"2022-01-10T17:06:09.057028Z","shell.execute_reply":"2022-01-10T17:06:16.784772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    train_data,\n    validation_data=valid_data,\n    epochs=epochs\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T17:06:16.787687Z","iopub.execute_input":"2022-01-10T17:06:16.788029Z","iopub.status.idle":"2022-01-10T17:52:11.163883Z","shell.execute_reply.started":"2022-01-10T17:06:16.78798Z","shell.execute_reply":"2022-01-10T17:52:11.162717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save(\"testmodel0.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-01-10T17:52:11.168556Z","iopub.execute_input":"2022-01-10T17:52:11.169369Z","iopub.status.idle":"2022-01-10T17:52:11.234054Z","shell.execute_reply.started":"2022-01-10T17:52:11.169316Z","shell.execute_reply":"2022-01-10T17:52:11.233015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_model.trainable = True\n# Recompile the model to make the change effective.\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(1e-5),\n    loss=\"categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-10T17:52:11.235622Z","iopub.execute_input":"2022-01-10T17:52:11.235954Z","iopub.status.idle":"2022-01-10T17:52:11.27983Z","shell.execute_reply.started":"2022-01-10T17:52:11.235906Z","shell.execute_reply":"2022-01-10T17:52:11.278735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    train_data,\n    validation_data=valid_data,\n    epochs=epochs,\n    use_multiprocessing=True,\n    workers=-1,\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T17:52:11.282631Z","iopub.execute_input":"2022-01-10T17:52:11.283058Z","iopub.status.idle":"2022-01-10T18:38:01.103396Z","shell.execute_reply.started":"2022-01-10T17:52:11.283011Z","shell.execute_reply":"2022-01-10T18:38:01.102426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save(\"testmodel1.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-01-10T18:38:01.105801Z","iopub.execute_input":"2022-01-10T18:38:01.106877Z","iopub.status.idle":"2022-01-10T18:38:01.164153Z","shell.execute_reply.started":"2022-01-10T18:38:01.106816Z","shell.execute_reply":"2022-01-10T18:38:01.16311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing Code","metadata":{}},{"cell_type":"code","source":"from keras.models import load_model\ncus_mod=load_model('./testmodel1.h5')","metadata":{"execution":{"iopub.status.busy":"2022-01-10T18:46:38.269316Z","iopub.execute_input":"2022-01-10T18:46:38.269609Z","iopub.status.idle":"2022-01-10T18:46:38.877688Z","shell.execute_reply.started":"2022-01-10T18:46:38.269577Z","shell.execute_reply":"2022-01-10T18:46:38.876674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence1='Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.'\nsentence2='Deep learning is a kind of machine learning where a computer analyzes algorithms and their results to \"learn\" ways of improving processes and creating new ones.'\ns3 = 'Deep learning can be considered as a subset of machine learning. It is a field that is based on learning and improving on its own by examining computer algorithms.'\ns4 = 'Deep learning is a process of learing cooking.'","metadata":{"execution":{"iopub.status.busy":"2022-01-10T18:55:24.33251Z","iopub.execute_input":"2022-01-10T18:55:24.332822Z","iopub.status.idle":"2022-01-10T18:55:24.338868Z","shell.execute_reply.started":"2022-01-10T18:55:24.332775Z","shell.execute_reply":"2022-01-10T18:55:24.337913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence_pairs = np.array([[str(sentence1), str(s4)]])\ntest_data = BertSemanticDataGenerator(\n        sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n    )\n\npro=cus_mod.predict(test_data)\nprint(pro)\n# [\"contradiction\", \"entailment\", \"neutral\"]","metadata":{"execution":{"iopub.status.busy":"2022-01-10T18:56:06.058173Z","iopub.execute_input":"2022-01-10T18:56:06.058827Z","iopub.status.idle":"2022-01-10T18:56:09.560203Z","shell.execute_reply.started":"2022-01-10T18:56:06.058791Z","shell.execute_reply":"2022-01-10T18:56:09.559275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-10T19:01:43.740905Z","iopub.execute_input":"2022-01-10T19:01:43.741248Z","iopub.status.idle":"2022-01-10T19:01:44.273192Z","shell.execute_reply.started":"2022-01-10T19:01:43.741189Z","shell.execute_reply":"2022-01-10T19:01:44.272223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the model trained for 200 total epochs loss curves\npd.DataFrame(history.history).plot()\nplt.ylabel(\"loss\")\nplt.xlabel(\"epochs\"); # note: epochs will only show 100 since we overrid the history variable","metadata":{"execution":{"iopub.status.busy":"2022-01-10T19:03:34.454878Z","iopub.execute_input":"2022-01-10T19:03:34.455181Z","iopub.status.idle":"2022-01-10T19:03:34.803404Z","shell.execute_reply.started":"2022-01-10T19:03:34.455142Z","shell.execute_reply":"2022-01-10T19:03:34.802014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}